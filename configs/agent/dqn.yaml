# DQN Agent Configuration
# All parameters for DQNAgent initialization

# Network architecture
hidden_layers: [512, 256, 128]

# Learning parameters
learning_rate: 0.0001
gamma: 0.95
tau: 0.005  # Soft update parameter for target network

# Exploration parameters
epsilon_start: 1.0
epsilon_end: 0.05
epsilon_decay: 0.999
epsilon_decay_strategy: "exponential"  # 'exponential' or 'linear'

# Replay buffer
buffer_size: 100000
batch_size: 64
min_buffer_size: 1000  # Minimum experiences before training starts
replay_buffer_size: 100000  # Alias for buffer_size (for backward compatibility)

# Training configuration
target_update_freq: 1  # Update target network every N episodes
gradient_clip_norm: 1.0
prioritized_replay: false
alpha: 0.6  # Prioritized replay alpha (only used if prioritized_replay=true)
beta: 0.4  # Prioritized replay beta (only used if prioritized_replay=true)

# Network enhancements
double_dqn: true
dueling_dqn: false

# Network regularization
dropout: 0.2
activation: "relu"  # 'relu', 'leaky_relu', 'elu', 'selu'

# Device and reproducibility
device: "cpu"  # 'cpu' or 'cuda'
seed: null  # Random seed (null for no seed)
