# DQN Training Configuration for Sevens RL

defaults:
  - default

# Override experiment name
experiment:
  name: "sevens_dqn"

# DQN-specific training parameters
training:
  num_episodes: 50000
  batch_size: 128
  learning_rate: 0.0001
  gamma: 0.95
  epsilon_start: 1.0
  epsilon_end: 0.05
  epsilon_decay: 0.999
  target_update_freq: 20
  replay_buffer_size: 100000
  min_replay_size: 1000  # Start training after this many transitions

# DQN network architecture
network:
  hidden_layers: [512, 256, 128]
  activation: "relu"
  dropout: 0.2
  dueling: false  # Use dueling DQN architecture

# Training optimizations
training_opts:
  gradient_clip: 1.0
  prioritized_replay: false
  double_dqn: true
